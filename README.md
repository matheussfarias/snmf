# Semi-Nonnegative Matrix Factorization for Crossbars: Using Fewer Memristors on DNNs

We propose a semi-nonnegative matrix factorization (Semi-NMF) for mapping deep neural network (DNN) weights into resistive compute-in-memory crossbars. There is no device with negative resistance. Thus, we require at least two crossbars to represent a single DNN weight matrix -- one for nonpositive and the other for nonnegative values. With Semi-NMF, we only double the mixed-sign crossbar, which is chosen to be the smallest. This reduces memristor count, greatly decreasing area with a potential DNN accuracy increase.

The method is evaluated on Gaussian matrices that behave similarly to DNN weights. We achieved up to 62.5\% of area and 68.75\% of parameter count savings with a Frobenius norm error of 19\% larger than the optimal factorization scenario. This novel weight allocation algorithm promotes area-efficient crossbars for DNNs.


## How to use the code
First run `Install.m` to define the path with all useful code.

Then, the main code is `test_semiNMF.m`, where we can find all the results of the final report. All the plot figures are contained in `plots/` folder and were generated by the `plots.py` file.

**Matheus Farias**